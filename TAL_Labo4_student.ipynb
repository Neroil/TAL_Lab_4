{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# Cours TAL - Laboratoire 4<br/>Reconnaissance des entités nommées\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "L'objectif de ce travail est de comparer la reconnaissance des entités nommées (*named entity recognition*, NER) faite par quatre systèmes : NLTK, spaCy (deux modèles, 'en_core_web_sm' et 'en_core_web_lg'), et DistilBERT/NER.  Les données de test en anglais vous sont fournies sur Cyberlearn au format CoNLL.  Pour comparer les systèmes, on utilise la macro-moyenne des scores f1 pour chaque étiquette.  Vous pouvez concevoir l'ensemble du projet par vous-mêmes, ou suivre les indications suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NER avec spaCy et NLTK sur un texte court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:36:43.235577Z",
     "start_time": "2025-04-10T11:36:42.265635Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:36:43.254263Z",
     "start_time": "2025-04-10T11:36:43.249884Z"
    }
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm \n",
    "#!python -m spacy download en_core_web_lg\n",
    "# exécuter la ligne ci-dessus une fois, si nécessaire, idem pour en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:36:44.385545Z",
     "start_time": "2025-04-10T11:36:43.393429Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:36:44.405709Z",
     "start_time": "2025-04-10T11:36:44.401749Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_text = \"Reinhold Messner made a solo ascent of Mount Everest and was later a member of the European Parliament.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Veuillez traiter ce texte avec la pipeline 'nlp', et pour chaque entité nommée trouvée veuillez afficher les mots qui la composent et son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:36:45.820059Z",
     "start_time": "2025-04-10T11:36:45.802810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity: Reinhold Messner, Type: ORG\n",
      "Named Entity: Mount Everest, Type: LOC\n",
      "Named Entity: the European Parliament, Type: ORG\n"
     ]
    }
   ],
   "source": [
    "treated_text = nlp(raw_text)\n",
    "\n",
    "for ent in treated_text.ents:\n",
    "    \n",
    "    print(f\"Named Entity: {ent.text}, Type: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:36:47.246986Z",
     "start_time": "2025-04-10T11:36:46.651825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/neroil/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/neroil/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/neroil/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/neroil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker') \n",
    "nltk.download('words') \n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "# exécuter les deux lignes ci-dessus une fois, si nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Veuillez effectuer avec NLTK la tokenization, le POS tagging et le *NE chunking* de `raw_text` (voir la [documentation NLTK](https://www.nltk.org/api/nltk.chunk.ne_chunk.html#nltk.chunk.ne_chunk)).  Veuillez afficher le résultat et indiquer son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:36:51.165301Z",
     "start_time": "2025-04-10T11:36:50.230070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.tree.Tree'>\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1008.0,168.0\" width=\"1008px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"7.93651%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Reinhold</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.96825%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.14286%\" x=\"7.93651%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Messner</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.5079%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.7619%\" x=\"15.0794%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">made</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.4603%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.1746%\" x=\"19.8413%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.4286%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.7619%\" x=\"23.0159%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">solo</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.3968%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.34921%\" x=\"27.7778%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ascent</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.9524%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.1746%\" x=\"34.127%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.7143%\" y1=\"20px\" y2=\"48px\" /><svg width=\"12.6984%\" x=\"37.3016%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"43.75%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Mount</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"56.25%\" x=\"43.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Everest</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.875%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.6508%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.96825%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.9841%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.96825%\" x=\"53.9683%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.9524%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.55556%\" x=\"57.9365%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">later</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.7143%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.1746%\" x=\"63.4921%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.0794%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.34921%\" x=\"66.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">member</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.8413%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.1746%\" x=\"73.0159%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.6032%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.96825%\" x=\"76.1905%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.1746%\" y1=\"20px\" y2=\"48px\" /><svg width=\"17.4603%\" x=\"80.1587%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">European</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"20px\" y2=\"48px\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Parliament</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.8889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.38095%\" x=\"97.619%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.8095%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Reinhold', 'NNP')]), Tree('PERSON', [('Messner', 'NNP')]), ('made', 'VBD'), ('a', 'DT'), ('solo', 'JJ'), ('ascent', 'NN'), ('of', 'IN'), Tree('PERSON', [('Mount', 'NNP'), ('Everest', 'NNP')]), ('and', 'CC'), ('was', 'VBD'), ('later', 'RB'), ('a', 'DT'), ('member', 'NN'), ('of', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('European', 'NNP'), ('Parliament', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the raw text\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# chunking\n",
    "chunked = nltk.chunk.ne_chunk(tagged_tokens, binary=False)\n",
    "\n",
    "print(type(chunked))\n",
    "chunked\n",
    "\n",
    "\n",
    "\n",
    "## https://www.nltk.org/book/ch03.html#sec-accessing-text\n",
    "\n",
    "##  nltk.chunk.ne_chunk(tagged_tokens, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c.** Veuillez afficher, pour chaque entité nommée, les mots qui la composent et son type.  Vous pouvez parcourir le résultat précédent avec une boucle `for`, et déterminer si un noeud possède une étiquette avec la fonction `hasattr(noeud, 'label')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:37:51.213504Z",
     "start_time": "2025-04-10T11:37:51.207830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity: Reinhold, Type: PERSON\n",
      "Named Entity: Messner, Type: PERSON\n",
      "Token: made, Type: VBD\n",
      "Token: a, Type: DT\n",
      "Token: solo, Type: JJ\n",
      "Token: ascent, Type: NN\n",
      "Token: of, Type: IN\n",
      "Named Entity: Mount Everest, Type: PERSON\n",
      "Token: and, Type: CC\n",
      "Token: was, Type: VBD\n",
      "Token: later, Type: RB\n",
      "Token: a, Type: DT\n",
      "Token: member, Type: NN\n",
      "Token: of, Type: IN\n",
      "Token: the, Type: DT\n",
      "Named Entity: European Parliament, Type: ORGANIZATION\n",
      "Token: ., Type: .\n"
     ]
    }
   ],
   "source": [
    "for noeud in chunked:\n",
    "    if hasattr(noeud, 'label'):\n",
    "        print(f\"Named Entity: {' '.join(c[0] for c in noeud)}, Type: {noeud.label()}\")\n",
    "    else:\n",
    "        print(f\"Token: {noeud[0]}, Type: {noeud[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1d.** À ce stade, que pensez-vous de la qualité des résultats de chaque système ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:37:53.014227Z",
     "start_time": "2025-04-10T11:37:53.010383Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "# La première erreur -> Reinhold Messner est considéré comme deux personnes au lieu d'une seule\n",
    "# La seconde erreur -> Mount Everest est un nom propre, mais pas une personne\n",
    "# Mais c'est la seule erreur de tagging, donc c'est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prise en main des données de test\n",
    "\n",
    "**2a.** Quel est le format du fichier `eng.test.a.conll` ?  Quelle information contient chaque colonne ?  Quel est le format des tags NE ?\n",
    "\n",
    "Note : ce fichier fait partie des données de test pour la NER sur l'anglais de la conférence [CoNLL](https://www.clips.uantwerpen.be/pages/past-workshops) 2003. On peut lire [ici](https://www.clips.uantwerpen.be/conll2003/ner/) la description de la tâche et les scores obtenus.  On peut trouver une copie des données [ici](https://sourceforge.net/p/text-analysis/svn/1243/tree/text-analysis/trunk/Corpora/CoNLL/2003/) ou [ici](https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003).  Les textes proviennent du [corpus Reuters](http://trec.nist.gov/data/reuters/reuters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:37:54.257174Z",
     "start_time": "2025-04-10T11:37:54.252659Z"
    }
   },
   "outputs": [],
   "source": [
    "# eng.test.a.conll est un fichier de format CoNLL. Les colonnes:\n",
    "# - 1 : le mot\n",
    "# - 2 : le tag du mot\n",
    "# - 3 : étiquette de typologie du chunk\n",
    "# - 4 : le format IOB\n",
    "\n",
    "# https://hal.science/hal-02567769v1/document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Veuillez charger les données de `eng.test.a.conll` grâce à la classe `ConllCorpusReader` de NLTK vue dans les labos précédents (voir [documentation](https://www.nltk.org/api/nltk.corpus.reader.conll.html#nltk.corpus.reader.conll.ConllCorpusReader)). Veuillez lire les colonnes qui contiennent les tokens ('words'), les POS tags ('pos') et les informations sur les entités nommées ('chunk') et afficher les quatre premières phrases, accessibles via la méthode `.iob_sents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:37:56.564377Z",
     "start_time": "2025-04-10T11:37:56.551400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('CRICKET', 'NNP', 'O'), ('-', ':', 'O'), ('LEICESTERSHIRE', 'NNP', 'I-ORG'), ('TAKE', 'NNP', 'O'), ('OVER', 'IN', 'O'), ('AT', 'NNP', 'O'), ('TOP', 'NNP', 'O'), ('AFTER', 'NNP', 'O'), ('INNINGS', 'NNP', 'O'), ('VICTORY', 'NN', 'O'), ('.', '.', 'O')]\n",
      "Sentence 2: [('LONDON', 'NNP', 'I-LOC'), ('1996-08-30', 'CD', 'O')]\n",
      "Sentence 3: [('West', 'NNP', 'I-MISC'), ('Indian', 'NNP', 'I-MISC'), ('all-rounder', 'NN', 'O'), ('Phil', 'NNP', 'I-PER'), ('Simmons', 'NNP', 'I-PER'), ('took', 'VBD', 'O'), ('four', 'CD', 'O'), ('for', 'IN', 'O'), ('38', 'CD', 'O'), ('on', 'IN', 'O'), ('Friday', 'NNP', 'O'), ('as', 'IN', 'O'), ('Leicestershire', 'NNP', 'I-ORG'), ('beat', 'VBD', 'O'), ('Somerset', 'NNP', 'I-ORG'), ('by', 'IN', 'O'), ('an', 'DT', 'O'), ('innings', 'NN', 'O'), ('and', 'CC', 'O'), ('39', 'CD', 'O'), ('runs', 'NNS', 'O'), ('in', 'IN', 'O'), ('two', 'CD', 'O'), ('days', 'NNS', 'O'), ('to', 'TO', 'O'), ('take', 'VB', 'O'), ('over', 'IN', 'O'), ('at', 'IN', 'O'), ('the', 'DT', 'O'), ('head', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('county', 'NN', 'O'), ('championship', 'NN', 'O'), ('.', '.', 'O')]\n",
      "Sentence 4: [('Their', 'PRP$', 'O'), ('stay', 'NN', 'O'), ('on', 'IN', 'O'), ('top', 'NN', 'O'), (',', ',', 'O'), ('though', 'RB', 'O'), (',', ',', 'O'), ('may', 'MD', 'O'), ('be', 'VB', 'O'), ('short-lived', 'JJ', 'O'), ('as', 'IN', 'O'), ('title', 'NN', 'O'), ('rivals', 'NNS', 'O'), ('Essex', 'NNP', 'I-ORG'), (',', ',', 'O'), ('Derbyshire', 'NNP', 'I-ORG'), ('and', 'CC', 'O'), ('Surrey', 'NNP', 'I-ORG'), ('all', 'DT', 'O'), ('closed', 'VBD', 'O'), ('in', 'RP', 'O'), ('on', 'IN', 'O'), ('victory', 'NN', 'O'), ('while', 'IN', 'O'), ('Kent', 'NNP', 'I-ORG'), ('made', 'VBD', 'O'), ('up', 'RP', 'O'), ('for', 'IN', 'O'), ('lost', 'VBN', 'O'), ('time', 'NN', 'O'), ('in', 'IN', 'O'), ('their', 'PRP$', 'O'), ('rain-affected', 'JJ', 'O'), ('match', 'NN', 'O'), ('against', 'IN', 'O'), ('Nottinghamshire', 'NNP', 'I-ORG'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "test_corpus = ConllCorpusReader('.', 'eng.test.a.conll', ['words', 'pos', 'ignore','chunk'])\n",
    "\n",
    "iob_sentences = test_corpus.iob_sents()\n",
    "\n",
    "for i, sentence in enumerate(iob_sentences[1:5]): #La première phrase étant vide\n",
    "    print(f\"Sentence {i + 1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Veuillez préparer les données pour le test, en ne gardant que les phrases ayant au moins trois (3) tokens (pas 0, 1, 2) :\n",
    "\n",
    "* une variable `test_tokens` contiendra les tokens groupés par phrase (liste de listes de strings)\n",
    "* une variable `test_tags` contiendra tous les tags NE en une seule liste (en vue de l'évaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2970\n",
      "50817\n",
      "[['CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.'], ['West', 'Indian', 'all-rounder', 'Phil', 'Simmons', 'took', 'four', 'for', '38', 'on', 'Friday', 'as', 'Leicestershire', 'beat', 'Somerset', 'by', 'an', 'innings', 'and', '39', 'runs', 'in', 'two', 'days', 'to', 'take', 'over', 'at', 'the', 'head', 'of', 'the', 'county', 'championship', '.'], ['Their', 'stay', 'on', 'top', ',', 'though', ',', 'may', 'be', 'short-lived', 'as', 'title', 'rivals', 'Essex', ',', 'Derbyshire', 'and', 'Surrey', 'all', 'closed', 'in', 'on', 'victory', 'while', 'Kent', 'made', 'up', 'for', 'lost', 'time', 'in', 'their', 'rain-affected', 'match', 'against', 'Nottinghamshire', '.']]\n"
     ]
    }
   ],
   "source": [
    "# TODO Fix les sentences, on en print BEAUCOUP trop ! \n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "saved_sentences = [sent for sent in test_corpus.iob_sents() if len(sent) >= 3]\n",
    "\n",
    "for sent in saved_sentences:\n",
    "    sent_token = []\n",
    "    for token, tag, chunk in sent:\n",
    "        sent_token.append(token)\n",
    "        test_tags.append(chunk)\n",
    "    test_tokens.append(sent_token)\n",
    "\n",
    "print(len(test_tokens))\n",
    "print(sum(len(sent) for sent in test_tokens))\n",
    "print(test_tokens[:3])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Combien d'occurrences de tags contient `test_tags`?  Combien de tags différents y a-t-il, et lesquels sont-ils ?  Combien il y a d'occurrences de tags de chaque type ?  Combien de phrases y a-t-il dans `test_tokens` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[('I-LOC', 1938), ('O', 42474), ('I-ORG', 2080), ('I-PER', 3097), ('I-MISC', 1228)]\n",
      "Nombre d’apparition des tags total : 50817\n"
     ]
    }
   ],
   "source": [
    "different_tags = set(test_tags)\n",
    "different_tags_count = [(tag, test_tags.count(tag)) for tag in different_tags]\n",
    "print(len(different_tags))\n",
    "print(different_tags_count)\n",
    "## count total\n",
    "print(f\"Nombre d’apparition des tags total : {sum([count for tag, count in different_tags_count])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performances de NLTK pour la NER\n",
    "\n",
    "**3a.** Le NER de NLTK a un jeu de tags différents de celui des données de test.  Veuillez chercher les informations pour compléter la fonction suivante qui convertit chaque tag du NER de NLTK vers le tag correspondant pour les données de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nltk_conll(nltk_tag):\n",
    "    corresp = {\n",
    "        'O': 'O',  \n",
    "        'ORGANIZATION': 'I-ORG',  \n",
    "        'PERSON': 'I-PER',  \n",
    "        'LOCATION': 'I-LOC',  \n",
    "        'GPE': 'I-LOC',  \n",
    "        'FACILITY': 'I-MISC',  \n",
    "        'MISC': 'I-MISC'  \n",
    "    }\n",
    "    return corresp.get(nltk_tag, 'O') # Default to O just in case \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez exécuter la NER de NLTK sur chacune des phrases de `test_tokens`, ce qui assure que NLTK aura la même tokenisation que les données de référence.  Veuillez stocker les tags dans une liste unique appelée `nltk_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50817\n"
     ]
    }
   ],
   "source": [
    "print(len(test_tokens))\n",
    "tagged_sentences = nltk.pos_tag_sents(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m nltk_tags = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_tokens:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     chunks = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mne_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m subtree \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(subtree, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/chunk/__init__.py:192\u001b[39m, in \u001b[36mne_chunk\u001b[39m\u001b[34m(tagged_tokens, binary)\u001b[39m\n\u001b[32m    190\u001b[39m     chunker = ne_chunker(fmt=\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     chunker = \u001b[43mne_chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chunker.parse(tagged_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/chunk/__init__.py:174\u001b[39m, in \u001b[36mne_chunker\u001b[39m\u001b[34m(fmt)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mne_chunker\u001b[39m(fmt=\u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Load NLTK's currently recommended named entity chunker.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMaxent_NE_Chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/chunk/named_entity.py:330\u001b[39m, in \u001b[36mMaxent_NE_Chunker.__init__\u001b[39m\u001b[34m(self, fmt)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28mself\u001b[39m._fmt = fmt\n\u001b[32m    329\u001b[39m \u001b[38;5;28mself\u001b[39m._tab_dir = find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchunkers/maxent_ne_chunker_tab/english_ace_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfmt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/chunk/named_entity.py:335\u001b[39m, in \u001b[36mMaxent_NE_Chunker.load_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaxent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinaryMaxentFeatureEncoding, load_maxent_params\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     wgt, mpg, lab, aon = \u001b[43mload_maxent_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tab_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     mc = MaxentClassifier(\n\u001b[32m    337\u001b[39m         BinaryMaxentFeatureEncoding(lab, mpg, alwayson_features=aon), wgt\n\u001b[32m    338\u001b[39m     )\n\u001b[32m    339\u001b[39m     \u001b[38;5;28mself\u001b[39m._tagger = NEChunkParserTagger(classifier=mc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/classify/maxent.py:1572\u001b[39m, in \u001b[36mload_maxent_params\u001b[39m\u001b[34m(tab_dir)\u001b[39m\n\u001b[32m   1569\u001b[39m mdec = MaxentDecoder()\n\u001b[32m   1571\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtab_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/weights.txt\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m-> \u001b[39m\u001b[32m1572\u001b[39m     wgt = numpy.array(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(numpy.float64, \u001b[43mmdec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtxt2list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[32m   1574\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtab_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/mapping.tab\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1575\u001b[39m     mpg = mdec.tupkey2dict(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/tabdata.py:41\u001b[39m, in \u001b[36mTabDecoder.txt2list\u001b[39m\u001b[34m(self, f)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtxt2list\u001b[39m(\u001b[38;5;28mself\u001b[39m, f):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mrm_nl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/tabdata.py:41\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtxt2list\u001b[39m(\u001b[38;5;28mself\u001b[39m, f):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mrm_nl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/CoursTAL_new/lib/python3.11/site-packages/nltk/tabdata.py:10\u001b[39m, in \u001b[36mrm_nl\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Natural Language Toolkit: Encode/Decocode Data as Tab-files\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright (C) 2024 NLTK Project\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrm_nl\u001b[39m(s):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m s[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m:\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m s[:-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "nltk_tags = []\n",
    "\n",
    "for sentence in test_tokens:\n",
    "    chunks = nltk.ne_chunk(nltk.pos_tag(sentence))\n",
    "    for subtree in chunks:\n",
    "        if hasattr(subtree, 'label'):\n",
    "            nltk_tags += [subtree.label() for leaf in subtree.leaves()]\n",
    "        else:\n",
    "            nltk_tags.append('O')\n",
    "\n",
    "# Afficher le nombre total de tags et les 10 premiers\n",
    "print(f\"Nombre total de tags : {len(nltk_tags)}\")\n",
    "print(f\"Les 10 premiers tags : {nltk_tags[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in input: 1326723\n",
      "Total tags generated: 1326723\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum(len(sent) for sent in tagged_sentences)\n",
    "print(f\"Total tokens in input: {total_tokens}\")\n",
    "print(f\"Total tags generated: {len(nltk_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Veuillez convertir les tags de `nltk_tags` grâce à la fonction précédente, dans une liste appelée `nltk_tags_conv`.  Veuillez afficher le nombre total de tags et les dix premiers.  Vous pouvez plusieurs essais en changeant la fonction, pour aboutir à la conversion qui maximise le score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** Veuillez afficher le rapport d'évaluation de classification obtenu de Scikit-learn et la matrice de confusion pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performances de spaCy pour la NER\n",
    "\n",
    "**4a.** Le NER de spaCy a aussi un jeu de tags différents de celui des données de test.  Veuillez chercher les informations pour compléter la fonction suivante qui convertir chaque tag du NER de spaCy dans le tag correspondant pour les données de test.  Attention à la logique des conversions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_spacy_conll(spacy_tag):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Veuillez exécuter la NER de spaCy sur chacune des phrases de `test_tokens`, ce qui assure que spaCy aura la même tokenisation que les données de référence.  Veuillez stocker les tags dans une liste unique appelée `spacy_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez convertir les tags de `spacy_tags` grâce à la fonction précédente, dans une liste appelée `spacy_tags_conv`.  Veuillez afficher le nombre total de tags et les dix premiers.  Vous pouvez plusieurs essais en changeant la fonction, pour aboutir à la conversion qui maximise le score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4d.** Veuillez afficher le rapport d'évaluation de classification obtenu de Scikit-learn et la matrice de confusion pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4e.** Veuillez exécuter également le modèle 'en_core_web_lg' de spacy et afficher le rapport d'évaluation (il n'est pas demander d'afficher la matrice de confusion).  Vous pouvez recopier ici le minimum de code nécessaire à l'obtention des résultats, avec une nouvelle pipeline spaCy appelée 'nlp2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utilisation d'embeddings contextualisés fournis par BERT\n",
    "\n",
    "Dans cette section, vous allez évaluer un modèle basé sur le Transformer, qui fournit des embeddings contextualisés pour chaque token.  Ces embeddings sont ensuite utilisés par une couche finale (appelée _NER head_) pour assigner à chaque token une étiquette indiquant les entités nommées.  Le modèle et les instructions pour l'utiliser sont disponibles ici : https://huggingface.co/dslim/distilbert-NER (il s'agit d'une version de BERT \"distillée\" dans un modèle plus léger, suivi du _NER head_).\n",
    "\n",
    "**5a.** Prise en main du modèle : à l'aide des exemples fournis sur Hugging Face, veuillez appliquer DistilBERT_NER sur les 3 premières phrases des données contenues dans `test_tokens` (voir le point 2c) et afficher les résultats obtenus.  Quelles sont les différences avec les résultats de NLTK et de spaCy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les trois classes, créer le tokenizer, le modèle et la pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la pipeline sur les 3 premières phrases de test_tokens : afficher la phrase et le résultat de la pipeline.\n",
    "# Concaténer les tokens avec ' '.join(liste_de_tokens) avant de les donner à la pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Le modèle DistilBERT utilise son propre tokeniseur, c'est pourquoi les étiquettes indiquant les entités nommées sont parfois portées par plusieurs *subwords* composant un mot.  Il faut donc parfois agréger une liste d'étiquettes en une seule.  \n",
    "\n",
    "Veuillez écrire une fonction qui prend en entrée une liste d'étiquettes et retourne une seule étiquette : soit la première, soit celle qui est majoritaire.  Cette fonction sera utilisée plus loin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 2 (3300681988.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(tag_fusion(['O', 'B-PER', 'B-PER'], method='majority'))\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def tag_fusion(tag_list, method='first'): # ou method='majority'\n",
    "\n",
    "print(tag_fusion(['O', 'B-PER', 'B-PER'], method='majority'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** Veuillez afficher le `set` des tags apparaissant dans les résultats de DistilBERT_NER sur les 50 premières phrases du corpus.  Comment se comparent-ils aux tags des données de test CoNLL ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5d.**  Sur le modèle des points 3a et 4a, veuillez écrire une fonction qui convertit les tags générés par DistilBERT_NER aux tags des donnéees de test.  Vous utiliserez cette fonction plus loin.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bert_conll(bert_tag):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5e.** La pipeline DistilBERT_NER utilise son propre tokeniseur, qui ne peut pas être changé, car le modèle DistilBERT a été défini et entraîné avec lui.  On doit donc convertir les tokens du modèle à la même tokenisation que celle des données de test, en vue de l'évaluation.  Pour cela, on vous donne la fonction `convert_tokens` ainsi que du code pour la tester.  Veuillez étudier le code pour pouvoir l'utiliser plus bas, puis répondez aux questions ci-après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens(ref_tokens, bert_result, word_ids):\n",
    "    tokenized = tokenizer.tokenize(' '.join(ref_tokens))\n",
    "    all_tags = []\n",
    "    for tok, i in zip(tokenized, range(len(tokenized))):\n",
    "        label = [tag['entity'] for tag in bert_result if tag['index'] == i+1]\n",
    "        if label:\n",
    "            label = label[0]\n",
    "        else:\n",
    "            label = 'O'\n",
    "        if tok[:2] == '##' or word_ids[i] == word_ids[i-1]: \n",
    "            all_tags[-1].append(label)\n",
    "        else:\n",
    "            all_tags.append([label])\n",
    "    return [tag_fusion(taglist, method='first') for taglist in all_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de la fonction convert_tokens:\n",
    "i = 1 # choix de la phrase\n",
    "def print_len(l):\n",
    "    print(len(l), '--', l)\n",
    "print_len(test_tokens[i]) # Affichage 1\n",
    "tokenized_by_bert = tokenizer(test_tokens[i], add_special_tokens=False, is_split_into_words=True)\n",
    "print_len(tokenizer.convert_ids_to_tokens(tokenized_by_bert[\"input_ids\"]))  # Affichage 2\n",
    "print_len(tokenized_by_bert.word_ids())  # Affichage 3\n",
    "tagged_by_bert = distilbert_ner(' '.join(test_tokens[i]))\n",
    "print_len(tagged_by_bert)  # Affichage 4\n",
    "print_len(convert_tokens(test_tokens[i], tagged_by_bert, tokenized_by_bert.word_ids()))  # Affichage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions\n",
    "# 1. Que représentent les trois paramètres de convert_tokens ? Définissez-les comme dans une docstring.\n",
    "# 2. Que représentent les cinq lignes affichées par le code de test ?\n",
    "# 3. Quels sont les deux problèmes traités par la fonction convert_tokens ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5f.** Veuillez effectuer la reconnaissance des entités nommées avec la pipeline DistilBERT_NER et obtenir la liste finale de tags avec les noms convertis à ceux du jeu de référence grâce à `convert_bert_conll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 20 # se limiter aux j premières phrases sur les 2970 (utile pendant le développement, mais les utiliser toutes à la fin)\n",
    "bert_tags_conv = []\n",
    "\n",
    "print(len(bert_tags_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5g.** En comparant `test_tags` avec `bert_tags_conv`, veuillez afficher le rapport d'évaluation de la classification et la matrice de confusion, pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5h.** Laquelle des deux stratégies de `tag_fusion` (premier tag ou tag majoritaire) conduit à de meilleurs résultats ?  Veuillez effectuer l'expérience et indiquer simplement les scores obtenus et votre conclusion dans le champ suivant.  Pour la conclusion finale, gardez 'first'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion finale\n",
    "\n",
    "Veuillez comparer les scores des quatre modèles testés, en termes de **macro avg**.  Pourquoi ce score est-il le plus informatif ?  Veuillez indiquer également la taille des modèles spaCy évalués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo.** Veuillez nettoyer ce notebook en gardant seulement les résultats désirés, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CoursTAL new)",
   "language": "python",
   "name": "courstal_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
